import pandas as pd
import pyodbc
import csv
import datetime
from concurrent.futures import ThreadPoolExecutor, as_completed
import hashlib
import collections
import os
import re

def insert_row(row, table_name):
    server = 'VDI-RNA\\SQLEXPRESS' 
    database = 'RNA'
    trusted_connection = 'yes'
    driver = 'ODBC Driver 17 for SQL Server'  

    conn_str = f'DRIVER={driver};SERVER={server};DATABASE={database};Trusted_Connection={trusted_connection};'
    t=0
    while True:
        try:
            conn = pyodbc.connect(conn_str)
            cursor = conn.cursor()
            insert_query = f'''
            INSERT INTO {table_name} (
                [Identificativo Misura (CAR)], [Titolo Misura], [Tipo Misura], [Norma Misura], [COR], [Titolo Progetto], [Descrizione], 
                [Data Concessione], [Cup], [Atto Concessione], [Denominazione Beneficiario], [C.F. Beneficiario], [Dimensione Beneficiario], 
                [Regione], [Autorità Concedente], [Numero di riferimento della misura], [Identificativo componente], [Tipo procedimento], [Regolamento/Comunicazione], 
                [Obiettivo], [Settore di attività], [Strumento di aiuto], [Codice Strumento], [Elemento di aiuto], [Importo Nominale], [Codice Univoco]
            )
            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?);
            '''
            cursor.execute(insert_query, row)
            conn.commit()
            conn.close()
            break
        except Exception as e:
            t+=1
            print(t)

def calculate_hash(row):
    data_string = '|'.join(map(str, row))
    hash_object = hashlib.md5(data_string.encode())
    return hash_object.hexdigest()


def process_block(block):
    hash_list_local=[]
    i = 1
    for row in block:
        Codice_Univoco = calculate_hash(row)
        hash_list_local.append((row, Codice_Univoco))
    return hash_list_local

def read_csv_in_blocks(file_path, block_size=150000):
    with open(file_path, 'r', encoding='utf-8-sig') as csv_file:
        csv_reader = csv.reader(csv_file, delimiter='|', quotechar='"', quoting=csv.QUOTE_MINIMAL)
        next(csv_reader)
        block = []
        for index, row in enumerate(csv_reader):
            if index > 0 and index % block_size == 0:
                yield block
                block = []
            block.append(row)
        if block:
            yield block

def process_files(files):
    hash_list = []
    element_counters = collections.defaultdict(int)
    start_time = datetime.datetime.now()
    print(f"Inizio: {start_time.strftime('%H:%M:%S')}")

    with ThreadPoolExecutor(max_workers=4) as executor:
        futures = []
        for file in files:
            start_time1 = datetime.datetime.now()
            print(f"    nuovo file: {start_time1.strftime('%H:%M:%S')}")
            for block in read_csv_in_blocks(file):
                futures.append(executor.submit(process_block, block))

        for future in as_completed(futures):
            hash_list.extend(future.result())

    end_time = datetime.datetime.now()
    print(f"    Finito: {end_time.strftime('%H:%M:%S')}")
    print(f"    Righe Tot: {len(hash_list)}")
    hash=[tup[-1] for tup in hash_list]
    counter = collections.Counter(hash)
    row=[tup[0] for tup in hash_list]
    modified_hash_list = []
    element_counters = collections.defaultdict(int)

    for i in range(len(row)):

        if counter[hash[i]] > 1:
            element_counters[hash[i]] += 1
            if element_counters[hash[i]] == 1:
                row[i].append(hash[i])
            else:
                row[i].append(f"{hash[i]}_{element_counters[hash[i]]}")
        else:
            row[i].append(hash[i])
    with ThreadPoolExecutor(max_workers=4) as executor:
        futures = [executor.submit(insert_row, dato, "Open_Data") for dato in row]#.itertuples(index=False, name=None)]


    for future in futures:
        future.result()
        



def main():
    folder_path = "N:\\035-DEMINIMIS\\02-CONVERSIONE_DATI\\CSV_GABRIELE_OK"  
    file_pattern = re.compile(r'OpenData_Aiuti_(\d{4})_(\d{2})_?.*\.csv')

    files_by_month = collections.defaultdict(list)

    for filename in os.listdir(folder_path):
        match = file_pattern.match(filename)
        if match:
            year, month = match.groups()
            files_by_month[(year, month)].append(os.path.join(folder_path, filename))

    for (year, month), files in sorted(files_by_month.items()):
        print(f"    Mese : {year}-{month}")
        modified_hash_list = process_files(files)

if __name__ == "__main__":
    main()

